<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>2026-02-04 AI 리서치 브리핑</title>
  <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;600;700&family=Noto+Sans+KR:wght@400;500;700&display=swap" rel="stylesheet">
  <style>
    :root {
      --bg: #f6f1ea;
      --bg-accent: #e3efe9;
      --card: #ffffff;
      --text: #1c232b;
      --muted: #5e6b78;
      --accent: #00808f;
      --accent-2: #ff7a59;
      --border: #e2dcd5;
      --shadow: 0 16px 40px rgba(24, 32, 45, 0.08);
    }

    * {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      font-family: 'Noto Sans KR', sans-serif;
      color: var(--text);
      background: radial-gradient(circle at top left, var(--bg-accent), var(--bg)) no-repeat;
      min-height: 100vh;
    }

    .page {
      max-width: 1180px;
      margin: 0 auto;
      padding: 48px 20px 64px;
      position: relative;
    }

    .hero {
      display: grid;
      gap: 16px;
      margin-bottom: 36px;
    }

    .hero h1 {
      font-family: 'Space Grotesk', sans-serif;
      font-size: clamp(2.2rem, 3vw, 3.4rem);
      margin: 0;
      letter-spacing: -0.02em;
    }

    .hero p {
      margin: 0;
      color: var(--muted);
      font-size: 1.05rem;
      max-width: 640px;
    }

    .hero-meta {
      display: flex;
      flex-wrap: wrap;
      gap: 12px;
      font-size: 0.95rem;
    }

    .hero-meta span {
      background: var(--card);
      border: 1px solid var(--border);
      padding: 6px 12px;
      border-radius: 999px;
    }

    .nav-back {
      text-decoration: none;
      color: var(--accent);
      font-weight: 600;
      display: inline-flex;
      align-items: center;
      gap: 8px;
    }

    .section-block {
      margin-bottom: 42px;
    }

    .section-header {
      margin-bottom: 18px;
    }

    .section-header h2 {
      font-family: 'Space Grotesk', sans-serif;
      margin: 0 0 6px;
      font-size: 1.6rem;
    }

    .section-header p {
      margin: 0;
      color: var(--muted);
    }

    .card-grid {
      display: grid;
      gap: 18px;
      grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
    }

    .card-grid.scrollable {
      max-height: 520px;
      overflow-y: auto;
      padding-right: 6px;
    }

    .card-grid.scrollable::-webkit-scrollbar {
      width: 8px;
    }

    .card-grid.scrollable::-webkit-scrollbar-track {
      background: rgba(0, 0, 0, 0.04);
      border-radius: 999px;
    }

    .card-grid.scrollable::-webkit-scrollbar-thumb {
      background: rgba(0, 128, 143, 0.35);
      border-radius: 999px;
    }

    .card {
      background: var(--card);
      border-radius: 18px;
      padding: 20px 20px 18px;
      box-shadow: var(--shadow);
      border: 1px solid var(--border);
      display: flex;
      flex-direction: column;
      gap: 12px;
    }

    .card h3 {
      font-size: 1.1rem;
      margin: 0;
      line-height: 1.4;
    }

    .card-meta {
      display: flex;
      flex-wrap: wrap;
      gap: 8px;
      font-size: 0.8rem;
      color: var(--muted);
    }

    .pill {
      background: rgba(0, 128, 143, 0.12);
      color: var(--accent);
      padding: 4px 10px;
      border-radius: 999px;
      font-weight: 600;
    }

    .card-summary {
      margin: 0;
      color: var(--text);
      line-height: 1.6;
    }

    .card-translation {
      margin: 0;
      background: rgba(255, 122, 89, 0.12);
      color: #7a2c17;
      padding: 10px 12px;
      border-radius: 12px;
      font-size: 0.92rem;
    }

    .card-translation span {
      font-weight: 700;
      margin-right: 6px;
    }

    .card-link {
      margin-top: auto;
      text-decoration: none;
      color: var(--accent);
      font-weight: 600;
      display: inline-flex;
      align-items: center;
      gap: 6px;
    }

    .sources {
      background: var(--card);
      border: 1px solid var(--border);
      border-radius: 18px;
      padding: 22px;
      box-shadow: var(--shadow);
    }

    .sources h2 {
      font-family: 'Space Grotesk', sans-serif;
      margin: 0 0 12px;
    }

    .sources ul {
      list-style: none;
      padding: 0;
      margin: 0;
      display: grid;
      gap: 8px;
      grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
    }

    .sources a {
      text-decoration: none;
      color: var(--accent);
      font-weight: 600;
    }

    footer {
      margin-top: 42px;
      color: var(--muted);
      font-size: 0.9rem;
      display: flex;
      flex-direction: column;
      gap: 6px;
      text-align: center;
    }

    @media (max-width: 720px) {
      .page {
        padding: 36px 16px 56px;
      }

      .hero-meta {
        flex-direction: column;
        align-items: flex-start;
      }
    }
  </style>
</head>
<body>
  <div class="page">
    <header class="hero">
      <a class="nav-back" href="../index.html">← 전체 목록</a>
      <h1>2026-02-04 AI 리서치 브리핑</h1>
      <p>최신 VLM, sLLM, on-device AI 논문과 연구 블로그를 한눈에 정리합니다. 중복 기사 방지를 위해 URL 기준으로 추적합니다.</p>
      <div class="hero-meta">
        <span>총 18건 요약</span>
        <span>자동 생성</span>
      </div>
    </header>

    
    <section class="section-block">
      <div class="section-header">
        <h2>VLM 업데이트</h2>
        <p>멀티모달 비전-언어 모델의 최신 논문과 리더보드 변화</p>
      </div>
      <div class="card-grid scrollable">
        
    <article class="card" data-item-url="https://huggingface.co/papers/2602.00919" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>Green-VLA: Staged Vision-Language-Action Model for Generalist Robots</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">Green-VLA: Staged Vision-Language-Action Model for Generalist Robots에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://huggingface.co/papers/2602.00919" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://huggingface.co/papers/2601.22060" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://huggingface.co/papers/2601.22060" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://huggingface.co/papers/2602.02185" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://huggingface.co/papers/2602.02185" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="http://arxiv.org/abs/2602.02465v1" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>MentisOculi: Revealing the Limits of Reasoning with Mental Imagery</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.</p>
      
      <a class="card-link" href="http://arxiv.org/abs/2602.02465v1" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="http://arxiv.org/abs/2602.02444v1" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>RANKVIDEO: Reasoning Reranking for Text-to-Video Retrieval</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">Reranking is a critical component of modern retrieval systems, which typically pair an efficient first-stage retriever with a more expressive model to refine results. While large reasoning models have driven rapid progress in text-centric reranking, reasoning-based reranking for video retrieval remains underexplored. To address this gap, we introduce RANKVIDEO, a reasoning-based reranker for video retrieval that explicitly reasons over query-video pairs using video content to assess relevance. RANKVIDEO is trained using a two-stage curriculum consisting of perception-grounded supervised fine-tuning followed by reranking training that combines pointwise, pairwise, and teacher confidence distillation objectives, and is supported by a data synthesis pipeline for constructing reasoning-intensive query-video pairs. Experiments on the large-scale MultiVENT 2.0 benchmark demonstrate that RANKVIDEO consistently improves retrieval performance within a two-stage framework, yielding an average improvement of 31% on nDCG@10 and outperforming text-only and vision-language reranking alternatives, while more efficient.</p>
      
      <a class="card-link" href="http://arxiv.org/abs/2602.02444v1" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://www.microsoft.com/en-us/research/blog/unirg-scaling-medical-imaging-report-generation-with-multimodal-reinforcement-learning" data-source="Microsoft Research Blog" data-item-type="news">
      <header>
        <h3>UniRG: Scaling medical imaging report generation with multimodal reinforcement learning</h3>
        <div class="card-meta">
          <span class="pill">News</span>
          <span class="source">Microsoft Research Blog</span>
        </div>
      </header>
      <p class="card-summary">UniRG: Scaling medical imaging report generation with multimodal reinforcement learning에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://www.microsoft.com/en-us/research/blog/unirg-scaling-medical-imaging-report-generation-with-multimodal-reinforcement-learning" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://www.microsoft.com/en-us/research/blog/multimodal-reinforcement-learning-with-agentic-verifier-for-ai-agents" data-source="Microsoft Research Blog" data-item-type="news">
      <header>
        <h3>Multimodal reinforcement learning with agentic verifier for AI agents</h3>
        <div class="card-meta">
          <span class="pill">News</span>
          <span class="source">Microsoft Research Blog</span>
        </div>
      </header>
      <p class="card-summary">Multimodal reinforcement learning with agentic verifier for AI agents에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://www.microsoft.com/en-us/research/blog/multimodal-reinforcement-learning-with-agentic-verifier-for-ai-agents" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  
      </div>
    </section>
  


    <section class="section-block">
      <div class="section-header">
        <h2>On-Device AI</h2>
        <p>디바이스 내 추론 및 엣지 최적화 동향</p>
      </div>
      <div class="card-grid scrollable">
        
    <article class="card" data-item-url="http://arxiv.org/abs/2602.02471v1" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>Multi-head automated segmentation by incorporating detection head into the contextual layer neural network</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">Deep learning based auto segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or hallucinations, in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented with inter-slice context integration and a parallel detection head, which jointly performs slice-level structure detection via a multi-layer perceptron and pixel-level segmentation through a context-enhanced stream. Detection outputs gate the segmentation predictions to suppress false positives in anatomically invalid slices, and training uses slice-wise Tversky loss to address class imbalance. Experiments on the Prostate-Anatomical-Edge-Cases dataset from The Cancer Imaging Archive demonstrate that the gated model substantially outperforms a non-gated segmentation-only baseline, achieving a mean Dice loss of $0.013 \pm 0.036$ versus $0.732 \pm 0.314$, with detection probabilities strongly correlated with anatomical presence, effectively eliminating spurious segmentations. In contrast, the non-gated model exhibited higher variability and persistent false positives across all slices. These results indicate that detection-based gating enhances robustness and anatomical plausibility in automated segmentation applications, reducing hallucinated predictions without compromising segmentation quality in valid slices, and offers a promising approach for improving the reliability of clinical radiotherapy auto-contouring workflows.</p>
      
      <a class="card-link" href="http://arxiv.org/abs/2602.02471v1" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="http://arxiv.org/abs/2503.07825v2" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>Helios 2.0: A Robust, Ultra-Low Power Gesture Recognition System Optimised for Event-Sensor based Wearables</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">We present an advance in wearable technology: a mobile-optimized, real-time, ultra-low-power event camera system that enables natural hand gesture control for smart glasses, dramatically improving user experience. While hand gesture recognition in computer vision has advanced significantly, critical challenges remain in creating systems that are intuitive, adaptable across diverse users and environments, and energy-efficient enough for practical wearable applications. Our approach tackles these challenges through carefully selected microgestures: lateral thumb swipes across the index finger (in both directions) and a double pinch between thumb and index fingertips. These human-centered interactions leverage natural hand movements, ensuring intuitive usability without requiring users to learn complex command sequences. To overcome variability in users and environments, we developed a novel simulation methodology that enables comprehensive domain sampling without extensive real-world data collection. Our power-optimised architecture maintains exceptional performance, achieving F1 scores above 80\% on benchmark datasets featuring diverse users and environments. The resulting models operate at just 6-8 mW when exploiting the Qualcomm Snapdragon Hexagon DSP, with our 2-channel implementation exceeding 70\% F1 accuracy and our 6-channel model surpassing 80\% F1 accuracy across all gesture classes in user studies. These results were achieved using only synthetic training data. This improves on the state-of-the-art for F1 accuracy by 20\% with a power reduction 25x when using DSP. This advancement brings deploying ultra-low-power vision systems in wearable devices closer and opens new possibilities for seamless human-computer interaction.</p>
      
      <a class="card-link" href="http://arxiv.org/abs/2503.07825v2" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://research.google/blog/small-models-big-results-achieving-superior-intent-extraction-through-decomposition" data-source="Google Research Blog" data-item-type="news">
      <header>
        <h3>January 22, 2026 Small models, big results: Achieving superior intent extraction through decomposition Generative AI &amp;#183; Machine Perception &amp;#183; Mobile Systems</h3>
        <div class="card-meta">
          <span class="pill">News</span>
          <span class="source">Google Research Blog</span>
        </div>
      </header>
      <p class="card-summary">January 22, 2026 Small models, big results: Achieving superior intent extraction through decomposition Generative AI &amp;#183; Machine Perception &amp;#183; Mobile Systems에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://research.google/blog/small-models-big-results-achieving-superior-intent-extraction-through-decomposition" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  
      </div>
    </section>
  

    <section class="section-block">
      <div class="section-header">
        <h2>AI 뉴스 &amp; 리서치</h2>
        <p>기업/연구기관의 주요 발표와 블로그 업데이트</p>
      </div>
      <div class="card-grid scrollable">
        
    <article class="card" data-item-url="https://huggingface.co/papers/2602.02276" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>Kimi K2.5: Visual Agentic Intelligence</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">Kimi K2.5: Visual Agentic Intelligence에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://huggingface.co/papers/2602.02276" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://huggingface.co/papers/2602.02084" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>Closing the Loop: Universal Repository Representation with RPG-Encoder</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">Closing the Loop: Universal Repository Representation with RPG-Encoder에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://huggingface.co/papers/2602.02084" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="http://arxiv.org/abs/2602.02493v1" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.</p>
      
      <a class="card-link" href="http://arxiv.org/abs/2602.02493v1" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://research.google/blog/collaborating-on-a-nationwide-randomized-study-of-ai-in-real-world-virtual-care" data-source="Google Research Blog" data-item-type="news">
      <header>
        <h3>February 3, 2026 Collaborating on a nationwide randomized study of AI in real-world virtual care Generative AI &amp;#183; Health &amp; Bioscience &amp;#183; Machine Intelligence</h3>
        <div class="card-meta">
          <span class="pill">News</span>
          <span class="source">Google Research Blog</span>
        </div>
      </header>
      <p class="card-summary">February 3, 2026 Collaborating on a nationwide randomized study of AI in real-world virtual care Generative AI &amp;#183; Health &amp; Bioscience &amp;#183; Machine Intelligence에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://research.google/blog/collaborating-on-a-nationwide-randomized-study-of-ai-in-real-world-virtual-care" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://research.google/blog/towards-a-science-of-scaling-agent-systems-when-and-why-agent-systems-work" data-source="Google Research Blog" data-item-type="news">
      <header>
        <h3>January 28, 2026 Towards a science of scaling agent systems: When and why agent systems work Generative AI &amp;#183; Machine Intelligence</h3>
        <div class="card-meta">
          <span class="pill">News</span>
          <span class="source">Google Research Blog</span>
        </div>
      </header>
      <p class="card-summary">January 28, 2026 Towards a science of scaling agent systems: When and why agent systems work Generative AI &amp;#183; Machine Intelligence에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://research.google/blog/towards-a-science-of-scaling-agent-systems-when-and-why-agent-systems-work" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://research.google/blog/atlas-practical-scaling-laws-for-multilingual-models" data-source="Google Research Blog" data-item-type="news">
      <header>
        <h3>January 27, 2026 ATLAS: Practical scaling laws for multilingual models Generative AI &amp;#183; Global &amp;#183; Machine Intelligence &amp;#183; Natural Language Processing</h3>
        <div class="card-meta">
          <span class="pill">News</span>
          <span class="source">Google Research Blog</span>
        </div>
      </header>
      <p class="card-summary">January 27, 2026 ATLAS: Practical scaling laws for multilingual models Generative AI &amp;#183; Global &amp;#183; Machine Intelligence &amp;#183; Natural Language Processing에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://research.google/blog/atlas-practical-scaling-laws-for-multilingual-models" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://research.google/blog/introducing-gist-the-next-stage-in-smart-sampling" data-source="Google Research Blog" data-item-type="news">
      <header>
        <h3>January 23, 2026 Introducing GIST: The next stage in smart sampling Algorithms &amp; Theory &amp;#183; Data Mining &amp; Modeling &amp;#183; Machine Intelligence</h3>
        <div class="card-meta">
          <span class="pill">News</span>
          <span class="source">Google Research Blog</span>
        </div>
      </header>
      <p class="card-summary">January 23, 2026 Introducing GIST: The next stage in smart sampling Algorithms &amp; Theory &amp;#183; Data Mining &amp; Modeling &amp;#183; Machine Intelligence에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://research.google/blog/introducing-gist-the-next-stage-in-smart-sampling" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://www.microsoft.com/en-us/research/blog" data-source="Microsoft Research Blog" data-item-type="news">
      <header>
        <h3>Microsoft Research blog</h3>
        <div class="card-meta">
          <span class="pill">News</span>
          <span class="source">Microsoft Research Blog</span>
        </div>
      </header>
      <p class="card-summary">Microsoft Research blog에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://www.microsoft.com/en-us/research/blog" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  
      </div>
    </section>
  

    <section class="sources">
      <h2>참고한 소스</h2>
      <ul>
        <li><a href="https://huggingface.co/papers" target="_blank" rel="noopener noreferrer">Hugging Face Papers</a></li>
<li><a href="https://export.arxiv.org/api/query?search_query=cat:cs.CV&amp;sortBy=lastUpdatedDate&amp;sortOrder=descending&amp;max_results=10" target="_blank" rel="noopener noreferrer">arXiv cs.CV (recent)</a></li>
<li><a href="https://paperswithcode.com/" target="_blank" rel="noopener noreferrer">Papers with Code</a></li>
<li><a href="https://research.google/blog/" target="_blank" rel="noopener noreferrer">Google Research Blog</a></li>
<li><a href="https://www.microsoft.com/en-us/research/blog/" target="_blank" rel="noopener noreferrer">Microsoft Research Blog</a></li>
<li><a href="https://www.qualcomm.com/research/artificial-intelligence" target="_blank" rel="noopener noreferrer">Qualcomm AI Research</a></li>
<li><a href="https://ai.google.dev/edge" target="_blank" rel="noopener noreferrer">Google AI Edge</a></li>
<li><a href="https://huggingface.co/spaces/opencompass/open_vlm_leaderboard" target="_blank" rel="noopener noreferrer">Open VLM Leaderboard</a></li>
      </ul>
    </section>

    <footer>
      <div>요약은 Gemini API를 사용하며, 실제 원문은 "원문 보기" 링크를 확인하세요.</div>
      <div>중복 방지 기준: URL 기준 + 최근 180일 기록 유지</div>
    </footer>
  </div>
</body>
</html>
