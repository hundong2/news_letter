<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>2026-02-27 AI 리서치 브리핑</title>
  <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;600;700&family=Noto+Sans+KR:wght@400;500;700&display=swap" rel="stylesheet">
  <style>
    :root {
      --bg: #f6f1ea;
      --bg-accent: #e3efe9;
      --card: #ffffff;
      --text: #1c232b;
      --muted: #5e6b78;
      --accent: #00808f;
      --accent-2: #ff7a59;
      --border: #e2dcd5;
      --shadow: 0 16px 40px rgba(24, 32, 45, 0.08);
    }

    * {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      font-family: 'Noto Sans KR', sans-serif;
      color: var(--text);
      background: radial-gradient(circle at top left, var(--bg-accent), var(--bg)) no-repeat;
      min-height: 100vh;
    }

    .page {
      max-width: 1180px;
      margin: 0 auto;
      padding: 48px 20px 64px;
      position: relative;
    }

    .hero {
      display: grid;
      gap: 16px;
      margin-bottom: 36px;
    }

    .hero h1 {
      font-family: 'Space Grotesk', sans-serif;
      font-size: clamp(2.2rem, 3vw, 3.4rem);
      margin: 0;
      letter-spacing: -0.02em;
    }

    .hero p {
      margin: 0;
      color: var(--muted);
      font-size: 1.05rem;
      max-width: 640px;
    }

    .hero-meta {
      display: flex;
      flex-wrap: wrap;
      gap: 12px;
      font-size: 0.95rem;
    }

    .hero-meta span {
      background: var(--card);
      border: 1px solid var(--border);
      padding: 6px 12px;
      border-radius: 999px;
    }

    .nav-back {
      text-decoration: none;
      color: var(--accent);
      font-weight: 600;
      display: inline-flex;
      align-items: center;
      gap: 8px;
    }

    .section-block {
      margin-bottom: 42px;
    }

    .section-header {
      margin-bottom: 18px;
    }

    .section-header h2 {
      font-family: 'Space Grotesk', sans-serif;
      margin: 0 0 6px;
      font-size: 1.6rem;
    }

    .section-header p {
      margin: 0;
      color: var(--muted);
    }

    .card-grid {
      display: grid;
      gap: 18px;
      grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
    }

    .card-grid.scrollable {
      max-height: 520px;
      overflow-y: auto;
      padding-right: 6px;
    }

    .card-grid.scrollable::-webkit-scrollbar {
      width: 8px;
    }

    .card-grid.scrollable::-webkit-scrollbar-track {
      background: rgba(0, 0, 0, 0.04);
      border-radius: 999px;
    }

    .card-grid.scrollable::-webkit-scrollbar-thumb {
      background: rgba(0, 128, 143, 0.35);
      border-radius: 999px;
    }

    .card {
      background: var(--card);
      border-radius: 18px;
      padding: 20px 20px 18px;
      box-shadow: var(--shadow);
      border: 1px solid var(--border);
      display: flex;
      flex-direction: column;
      gap: 12px;
    }

    .card h3 {
      font-size: 1.1rem;
      margin: 0;
      line-height: 1.4;
    }

    .card-meta {
      display: flex;
      flex-wrap: wrap;
      gap: 8px;
      font-size: 0.8rem;
      color: var(--muted);
    }

    .pill {
      background: rgba(0, 128, 143, 0.12);
      color: var(--accent);
      padding: 4px 10px;
      border-radius: 999px;
      font-weight: 600;
    }

    .card-summary {
      margin: 0;
      color: var(--text);
      line-height: 1.6;
    }

    .card-translation {
      margin: 0;
      background: rgba(255, 122, 89, 0.12);
      color: #7a2c17;
      padding: 10px 12px;
      border-radius: 12px;
      font-size: 0.92rem;
    }

    .card-translation span {
      font-weight: 700;
      margin-right: 6px;
    }

    .card-link {
      margin-top: auto;
      text-decoration: none;
      color: var(--accent);
      font-weight: 600;
      display: inline-flex;
      align-items: center;
      gap: 6px;
    }

    .sources {
      background: var(--card);
      border: 1px solid var(--border);
      border-radius: 18px;
      padding: 22px;
      box-shadow: var(--shadow);
    }

    .sources h2 {
      font-family: 'Space Grotesk', sans-serif;
      margin: 0 0 12px;
    }

    .sources ul {
      list-style: none;
      padding: 0;
      margin: 0;
      display: grid;
      gap: 8px;
      grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
    }

    .sources a {
      text-decoration: none;
      color: var(--accent);
      font-weight: 600;
    }

    footer {
      margin-top: 42px;
      color: var(--muted);
      font-size: 0.9rem;
      display: flex;
      flex-direction: column;
      gap: 6px;
      text-align: center;
    }

    @media (max-width: 720px) {
      .page {
        padding: 36px 16px 56px;
      }

      .hero-meta {
        flex-direction: column;
        align-items: flex-start;
      }
    }
  </style>
</head>
<body>
  <div class="page">
    <header class="hero">
      <a class="nav-back" href="../index.html">← 전체 목록</a>
      <h1>2026-02-27 AI 리서치 브리핑</h1>
      <p>최신 VLM, sLLM, on-device AI 논문과 연구 블로그를 한눈에 정리합니다. 중복 기사 방지를 위해 URL 기준으로 추적합니다.</p>
      <div class="hero-meta">
        <span>총 11건 요약</span>
        <span>자동 생성</span>
      </div>
    </header>

    
    <section class="section-block">
      <div class="section-header">
        <h2>VLM 업데이트</h2>
        <p>멀티모달 비전-언어 모델의 최신 논문과 리더보드 변화</p>
      </div>
      <div class="card-grid scrollable">
        
    <article class="card" data-item-url="https://huggingface.co/papers/2602.21818" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>SkyReels-V4: Multi-modal Video-Audio Generation, Inpainting and Editing model</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">SkyReels-V4: Multi-modal Video-Audio Generation, Inpainting and Editing model에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://huggingface.co/papers/2602.21818" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="http://arxiv.org/abs/2602.00288v3" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ .</p>
      
      <a class="card-link" href="http://arxiv.org/abs/2602.00288v3" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  
      </div>
    </section>
  



    <section class="section-block">
      <div class="section-header">
        <h2>AI 뉴스 &amp; 리서치</h2>
        <p>기업/연구기관의 주요 발표와 블로그 업데이트</p>
      </div>
      <div class="card-grid scrollable">
        
    <article class="card" data-item-url="https://huggingface.co/papers/2602.18283" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>HyTRec: A Hybrid Temporal-Aware Attention Architecture for Long Behavior Sequential Recommendation</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">HyTRec: A Hybrid Temporal-Aware Attention Architecture for Long Behavior Sequential Recommendation에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://huggingface.co/papers/2602.18283" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://huggingface.co/papers/2602.17602" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://huggingface.co/papers/2602.17602" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://huggingface.co/papers/2602.12160" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://huggingface.co/papers/2602.12160" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://huggingface.co/papers/2602.21534" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://huggingface.co/papers/2602.21534" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="http://arxiv.org/abs/2602.22212v1" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>Neu-PiG: Neural Preconditioned Grids for Fast Dynamic Surface Reconstruction on Long Sequences</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">Temporally consistent surface reconstruction of dynamic 3D objects from unstructured point cloud data remains challenging, especially for very long sequences. Existing methods either optimize deformations incrementally, risking drift and requiring long runtimes, or rely on complex learned models that demand category-specific training. We present Neu-PiG, a fast deformation optimization method based on a novel preconditioned latent-grid encoding that distributes spatial features parameterized on the position and normal direction of a keyframe surface. Our method encodes entire deformations across all time steps at various spatial scales into a multi-resolution latent grid, parameterized by the position and normal direction of a reference surface from a single keyframe. This latent representation is then augmented for time modulation and decoded into per-frame 6-DoF deformations via a lightweight multilayer perceptron (MLP). To achieve high-fidelity, drift-free surface reconstructions in seconds, we employ Sobolev preconditioning during gradient-based training of the latent space, completely avoiding the need for any explicit correspondences or further priors. Experiments across diverse human and animal datasets demonstrate that Neu-PiG outperforms state-the-art approaches, offering both superior accuracy and scalability to long sequences while running at least 60x faster than existing training-free methods and achieving inference speeds on the same order as heavy pretrained models.</p>
      
      <a class="card-link" href="http://arxiv.org/abs/2602.22212v1" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="http://arxiv.org/abs/2602.22209v1" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>WHOLE: World-Grounded Hand-Object Lifted from Egocentric Videos</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">Egocentric manipulation videos are highly challenging due to severe occlusions during interactions and frequent object entries and exits from the camera view as the person moves. Current methods typically focus on recovering either hand or object pose in isolation, but both struggle during interactions and fail to handle out-of-sight cases. Moreover, their independent predictions often lead to inconsistent hand-object relations. We introduce WHOLE, a method that holistically reconstructs hand and object motion in world space from egocentric videos given object templates. Our key insight is to learn a generative prior over hand-object motion to jointly reason about their interactions. At test time, the pretrained prior is guided to generate trajectories that conform to the video observations. This joint generative reconstruction substantially outperforms approaches that process hands and objects separately followed by post-processing. WHOLE achieves state-of-the-art performance on hand motion estimation, 6D object pose estimation, and their relative interaction reconstruction. Project website: https://judyye.github.io/whole-www</p>
      
      <a class="card-link" href="http://arxiv.org/abs/2602.22209v1" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="http://arxiv.org/abs/2602.22208v1" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>Solaris: Building a Multiplayer Video World Model in Minecraft</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">Existing action-conditioned video generation models (video world models) are limited to single-agent perspectives, failing to capture the multi-agent interactions of real-world environments. We introduce Solaris, a multiplayer video world model that simulates consistent multi-view observations. To enable this, we develop a multiplayer data system designed for robust, continuous, and automated data collection on video games such as Minecraft. Unlike prior platforms built for single-player settings, our system supports coordinated multi-agent interaction and synchronized videos + actions capture. Using this system, we collect 12.64 million multiplayer frames and propose an evaluation framework for multiplayer movement, memory, grounding, building, and view consistency. We train Solaris using a staged pipeline that progressively transitions from single-player to multiplayer modeling, combining bidirectional, causal, and Self Forcing training. In the final stage, we introduce Checkpointed Self Forcing, a memory-efficient Self Forcing variant that enables a longer-horizon teacher. Results show our architecture and training design outperform existing baselines. Through open-sourcing our system and models, we hope to lay the groundwork for a new generation of multi-agent world models.</p>
      
      <a class="card-link" href="http://arxiv.org/abs/2602.22208v1" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="http://arxiv.org/abs/2602.22197v1" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>Off-The-Shelf Image-to-Image Models Are All You Need To Defeat Image Protection Schemes</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">Advances in Generative AI (GenAI) have led to the development of various protection strategies to prevent the unauthorized use of images. These methods rely on adding imperceptible protective perturbations to images to thwart misuse such as style mimicry or deepfake manipulations. Although previous attacks on these protections required specialized, purpose-built methods, we demonstrate that this is no longer necessary. We show that off-the-shelf image-to-image GenAI models can be repurposed as generic ``denoisers&quot; using a simple text prompt, effectively removing a wide range of protective perturbations. Across 8 case studies spanning 6 diverse protection schemes, our general-purpose attack not only circumvents these defenses but also outperforms existing specialized attacks while preserving the image&#39;s utility for the adversary. Our findings reveal a critical and widespread vulnerability in the current landscape of image protection, indicating that many schemes provide a false sense of security. We stress the urgent need to develop robust defenses and establish that any future protection mechanism must be benchmarked against attacks from off-the-shelf GenAI models. Code is available in this repository: https://github.com/mlsecviswanath/img2imgdenoiser</p>
      
      <a class="card-link" href="http://arxiv.org/abs/2602.22197v1" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://www.microsoft.com/en-us/research/blog/corpgen-advances-ai-agents-for-real-work" data-source="Microsoft Research Blog" data-item-type="news">
      <header>
        <h3>CORPGEN advances AI agents for real work</h3>
        <div class="card-meta">
          <span class="pill">News</span>
          <span class="source">Microsoft Research Blog</span>
        </div>
      </header>
      <p class="card-summary">CORPGEN enables AI agents to manage dozens of interdependent tasks simultaneously in simulated workplace environments. It maintains performance under heavy multitasking, delivering up to 3.5x higher completion rates than leading baselines.</p>
      
      <a class="card-link" href="https://www.microsoft.com/en-us/research/blog/corpgen-advances-ai-agents-for-real-work" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  
      </div>
    </section>
  

    <section class="sources">
      <h2>참고한 소스</h2>
      <ul>
        <li><a href="https://huggingface.co/papers" target="_blank" rel="noopener noreferrer">Hugging Face Papers</a></li>
<li><a href="https://export.arxiv.org/api/query?search_query=cat:cs.CV&amp;sortBy=lastUpdatedDate&amp;sortOrder=descending&amp;max_results=10" target="_blank" rel="noopener noreferrer">arXiv cs.CV (recent)</a></li>
<li><a href="https://paperswithcode.com/" target="_blank" rel="noopener noreferrer">Papers with Code</a></li>
<li><a href="https://research.google/blog/" target="_blank" rel="noopener noreferrer">Google Research Blog</a></li>
<li><a href="https://www.microsoft.com/en-us/research/blog/" target="_blank" rel="noopener noreferrer">Microsoft Research Blog</a></li>
<li><a href="https://www.qualcomm.com/research/artificial-intelligence" target="_blank" rel="noopener noreferrer">Qualcomm AI Research</a></li>
<li><a href="https://ai.google.dev/edge" target="_blank" rel="noopener noreferrer">Google AI Edge</a></li>
<li><a href="https://huggingface.co/spaces/opencompass/open_vlm_leaderboard" target="_blank" rel="noopener noreferrer">Open VLM Leaderboard</a></li>
      </ul>
    </section>

    <footer>
      <div>요약은 Gemini API를 사용하며, 실제 원문은 "원문 보기" 링크를 확인하세요.</div>
      <div>중복 방지 기준: URL 기준 + 최근 180일 기록 유지</div>
    </footer>
  </div>
</body>
</html>
