<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>2026-02-18 AI 리서치 브리핑</title>
  <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;600;700&family=Noto+Sans+KR:wght@400;500;700&display=swap" rel="stylesheet">
  <style>
    :root {
      --bg: #f6f1ea;
      --bg-accent: #e3efe9;
      --card: #ffffff;
      --text: #1c232b;
      --muted: #5e6b78;
      --accent: #00808f;
      --accent-2: #ff7a59;
      --border: #e2dcd5;
      --shadow: 0 16px 40px rgba(24, 32, 45, 0.08);
    }

    * {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      font-family: 'Noto Sans KR', sans-serif;
      color: var(--text);
      background: radial-gradient(circle at top left, var(--bg-accent), var(--bg)) no-repeat;
      min-height: 100vh;
    }

    .page {
      max-width: 1180px;
      margin: 0 auto;
      padding: 48px 20px 64px;
      position: relative;
    }

    .hero {
      display: grid;
      gap: 16px;
      margin-bottom: 36px;
    }

    .hero h1 {
      font-family: 'Space Grotesk', sans-serif;
      font-size: clamp(2.2rem, 3vw, 3.4rem);
      margin: 0;
      letter-spacing: -0.02em;
    }

    .hero p {
      margin: 0;
      color: var(--muted);
      font-size: 1.05rem;
      max-width: 640px;
    }

    .hero-meta {
      display: flex;
      flex-wrap: wrap;
      gap: 12px;
      font-size: 0.95rem;
    }

    .hero-meta span {
      background: var(--card);
      border: 1px solid var(--border);
      padding: 6px 12px;
      border-radius: 999px;
    }

    .nav-back {
      text-decoration: none;
      color: var(--accent);
      font-weight: 600;
      display: inline-flex;
      align-items: center;
      gap: 8px;
    }

    .section-block {
      margin-bottom: 42px;
    }

    .section-header {
      margin-bottom: 18px;
    }

    .section-header h2 {
      font-family: 'Space Grotesk', sans-serif;
      margin: 0 0 6px;
      font-size: 1.6rem;
    }

    .section-header p {
      margin: 0;
      color: var(--muted);
    }

    .card-grid {
      display: grid;
      gap: 18px;
      grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
    }

    .card-grid.scrollable {
      max-height: 520px;
      overflow-y: auto;
      padding-right: 6px;
    }

    .card-grid.scrollable::-webkit-scrollbar {
      width: 8px;
    }

    .card-grid.scrollable::-webkit-scrollbar-track {
      background: rgba(0, 0, 0, 0.04);
      border-radius: 999px;
    }

    .card-grid.scrollable::-webkit-scrollbar-thumb {
      background: rgba(0, 128, 143, 0.35);
      border-radius: 999px;
    }

    .card {
      background: var(--card);
      border-radius: 18px;
      padding: 20px 20px 18px;
      box-shadow: var(--shadow);
      border: 1px solid var(--border);
      display: flex;
      flex-direction: column;
      gap: 12px;
    }

    .card h3 {
      font-size: 1.1rem;
      margin: 0;
      line-height: 1.4;
    }

    .card-meta {
      display: flex;
      flex-wrap: wrap;
      gap: 8px;
      font-size: 0.8rem;
      color: var(--muted);
    }

    .pill {
      background: rgba(0, 128, 143, 0.12);
      color: var(--accent);
      padding: 4px 10px;
      border-radius: 999px;
      font-weight: 600;
    }

    .card-summary {
      margin: 0;
      color: var(--text);
      line-height: 1.6;
    }

    .card-translation {
      margin: 0;
      background: rgba(255, 122, 89, 0.12);
      color: #7a2c17;
      padding: 10px 12px;
      border-radius: 12px;
      font-size: 0.92rem;
    }

    .card-translation span {
      font-weight: 700;
      margin-right: 6px;
    }

    .card-link {
      margin-top: auto;
      text-decoration: none;
      color: var(--accent);
      font-weight: 600;
      display: inline-flex;
      align-items: center;
      gap: 6px;
    }

    .sources {
      background: var(--card);
      border: 1px solid var(--border);
      border-radius: 18px;
      padding: 22px;
      box-shadow: var(--shadow);
    }

    .sources h2 {
      font-family: 'Space Grotesk', sans-serif;
      margin: 0 0 12px;
    }

    .sources ul {
      list-style: none;
      padding: 0;
      margin: 0;
      display: grid;
      gap: 8px;
      grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
    }

    .sources a {
      text-decoration: none;
      color: var(--accent);
      font-weight: 600;
    }

    footer {
      margin-top: 42px;
      color: var(--muted);
      font-size: 0.9rem;
      display: flex;
      flex-direction: column;
      gap: 6px;
      text-align: center;
    }

    @media (max-width: 720px) {
      .page {
        padding: 36px 16px 56px;
      }

      .hero-meta {
        flex-direction: column;
        align-items: flex-start;
      }
    }
  </style>
</head>
<body>
  <div class="page">
    <header class="hero">
      <a class="nav-back" href="../index.html">← 전체 목록</a>
      <h1>2026-02-18 AI 리서치 브리핑</h1>
      <p>최신 VLM, sLLM, on-device AI 논문과 연구 블로그를 한눈에 정리합니다. 중복 기사 방지를 위해 URL 기준으로 추적합니다.</p>
      <div class="hero-meta">
        <span>총 11건 요약</span>
        <span>자동 생성</span>
      </div>
    </header>

    
    <section class="section-block">
      <div class="section-header">
        <h2>VLM 업데이트</h2>
        <p>멀티모달 비전-언어 모델의 최신 논문과 리더보드 변화</p>
      </div>
      <div class="card-grid scrollable">
        
    <article class="card" data-item-url="https://huggingface.co/papers/2602.10809" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://huggingface.co/papers/2602.10809" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="http://arxiv.org/abs/2503.04641v3" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>Simulating the Real World: A Unified Survey of Multimodal Generative Models</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">Understanding and replicating the real world is a critical challenge in Artificial General Intelligence (AGI) research. To achieve this, many existing approaches, such as world models, aim to capture the fundamental principles governing the physical world, enabling more accurate simulations and meaningful interactions. However, current methods often treat different modalities, including 2D (images), videos, 3D, and 4D representations, as independent domains, overlooking their interdependencies. Additionally, these methods typically focus on isolated dimensions of reality without systematically integrating their connections. In this survey, we present a unified survey for multimodal generative models that investigate the progression of data dimensionality in real-world simulation. Specifically, this survey starts from 2D generation (appearance), then moves to video (appearance+dynamics) and 3D generation (appearance+geometry), and finally culminates in 4D generation that integrate all dimensions. To the best of our knowledge, this is the first attempt to systematically unify the study of 2D, video, 3D and 4D generation within a single framework. To guide future research, we provide a comprehensive review of datasets, evaluation metrics and future directions, and fostering insights for newcomers. This survey serves as a bridge to advance the study of multimodal generative models and real-world simulation within a unified framework.</p>
      
      <a class="card-link" href="http://arxiv.org/abs/2503.04641v3" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="http://arxiv.org/abs/2602.15018v1" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>Neurosim: A Fast Simulator for Neuromorphic Robot Perception</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">Neurosim is a fast, real-time, high-performance library for simulating sensors such as dynamic vision sensors, RGB cameras, depth sensors, and inertial sensors. It can also simulate agile dynamics of multi-rotor vehicles in complex and dynamic environments. Neurosim can achieve frame rates as high as ~2700 FPS on a desktop GPU. Neurosim integrates with a ZeroMQ-based communication library called Cortex to facilitate seamless integration with machine learning and robotics workflows. Cortex provides a high-throughput, low-latency message-passing system for Python and C++ applications, with native support for NumPy arrays and PyTorch tensors. This paper discusses the design philosophy behind Neurosim and Cortex. It demonstrates how they can be used to (i) train neuromorphic perception and control algorithms, e.g., using self-supervised learning on time-synchronized multi-modal data, and (ii) test real-time implementations of these algorithms in closed-loop. Neurosim and Cortex are available at https://github.com/grasp-lyrl/neurosim .</p>
      
      <a class="card-link" href="http://arxiv.org/abs/2602.15018v1" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  
      </div>
    </section>
  



    <section class="section-block">
      <div class="section-header">
        <h2>AI 뉴스 &amp; 리서치</h2>
        <p>기업/연구기관의 주요 발표와 블로그 업데이트</p>
      </div>
      <div class="card-grid scrollable">
        
    <article class="card" data-item-url="https://huggingface.co/papers/2602.13949" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>Experiential Reinforcement Learning</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">Experiential Reinforcement Learning에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://huggingface.co/papers/2602.13949" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://huggingface.co/papers/2602.14234" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://huggingface.co/papers/2602.14234" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://huggingface.co/papers/2602.14265" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>STATe-of-Thoughts: Structured Action Templates for Tree-of-Thoughts</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">STATe-of-Thoughts: Structured Action Templates for Tree-of-Thoughts에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://huggingface.co/papers/2602.14265" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://huggingface.co/papers/2602.14492" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>Query as Anchor: Scenario-Adaptive User Representation via Large Language Model</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">Query as Anchor: Scenario-Adaptive User Representation via Large Language Model에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://huggingface.co/papers/2602.14492" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="http://arxiv.org/abs/2602.15031v1" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">High-fidelity generative video editing has seen significant quality improvements by leveraging pre-trained video foundation models. However, their computational cost is a major bottleneck, as they are often designed to inefficiently process the full video context regardless of the inpainting mask&#39;s size, even for sparse, localized edits. In this paper, we introduce EditCtrl, an efficient video inpainting control framework that focuses computation only where it is needed. Our approach features a novel local video context module that operates solely on masked tokens, yielding a computational cost proportional to the edit size. This local-first generation is then guided by a lightweight temporal global context embedder that ensures video-wide context consistency with minimal overhead. Not only is EditCtrl 10 times more compute efficient than state-of-the-art generative editing methods, it even improves editing quality compared to methods designed with full-attention. Finally, we showcase how EditCtrl unlocks new capabilities, including multi-region editing with text prompts and autoregressive content propagation.</p>
      
      <a class="card-link" href="http://arxiv.org/abs/2602.15031v1" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="http://arxiv.org/abs/2602.15030v1" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>Image Generation with a Sphere Encoder</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">We introduce the Sphere Encoder, an efficient generative framework capable of producing images in a single forward pass and competing with many-step diffusion models using fewer than five steps. Our approach works by learning an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to the image space. Trained solely through image reconstruction losses, the model generates an image by simply decoding a random point on the sphere. Our architecture naturally supports conditional generation, and looping the encoder/decoder a few times can further enhance image quality. Across several datasets, the sphere encoder approach yields performance competitive with state of the art diffusions, but with a small fraction of the inference cost. Project page is available at https://sphere-encoder.github.io .</p>
      
      <a class="card-link" href="http://arxiv.org/abs/2602.15030v1" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="http://arxiv.org/abs/2506.17040v3" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>Stretching Beyond the Obvious: A Gradient-Free Framework to Unveil the Hidden Landscape of Visual Invariance</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">Uncovering which feature combinations are encoded by visual units is critical to understanding how images are transformed into representations that support recognition. While existing feature visualization approaches typically infer a unit&#39;s most exciting images, this is insufficient to reveal the manifold of transformations under which responses remain invariant, which is critical to generalization in vision. Here we introduce Stretch-and-Squeeze (SnS), a model-agnostic, gradient-free framework to systematically characterize a unit&#39;s maximally invariant stimuli, and its vulnerability to adversarial perturbations, in both biological and artificial visual systems. SnS frames these transformations as bi-objective optimization problems. To probe invariance, SnS seeks image perturbations that maximally alter (stretch) the representation of a reference stimulus in a given processing stage while preserving unit activation downstream (squeeze). To probe adversarial sensitivity, stretching and squeezing are reversed to maximally perturb unit activation while minimizing changes to the upstream representation. Applied to CNNs, SnS revealed invariant transformations that were farther from a reference image in pixel-space than those produced by affine transformations, while more strongly preserving the target unit&#39;s response. The discovered invariant images differed depending on the stage of the image representation used for optimization: pixel-level changes primarily affected luminance and contrast, while stretching mid- and late-layer representations mainly altered texture and pose. By measuring how well the hierarchical invariant images obtained for L2 robust networks were classified by humans and other observer networks, we discovered a substantial drop in their interpretability when the representation was stretched in deep layers, while the opposite trend was found for standard models.</p>
      
      <a class="card-link" href="http://arxiv.org/abs/2506.17040v3" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://research.google/blog/teaching-ai-to-read-a-map" data-source="Google Research Blog" data-item-type="news">
      <header>
        <h3>February 17, 2026 Teaching AI to read a map Machine Perception &amp;#183; Open Source Models &amp; Datasets</h3>
        <div class="card-meta">
          <span class="pill">News</span>
          <span class="source">Google Research Blog</span>
        </div>
      </header>
      <p class="card-summary">February 17, 2026 Teaching AI to read a map Machine Perception &amp;#183; Open Source Models &amp; Datasets에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://research.google/blog/teaching-ai-to-read-a-map" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  
      </div>
    </section>
  

    <section class="sources">
      <h2>참고한 소스</h2>
      <ul>
        <li><a href="https://huggingface.co/papers" target="_blank" rel="noopener noreferrer">Hugging Face Papers</a></li>
<li><a href="https://export.arxiv.org/api/query?search_query=cat:cs.CV&amp;sortBy=lastUpdatedDate&amp;sortOrder=descending&amp;max_results=10" target="_blank" rel="noopener noreferrer">arXiv cs.CV (recent)</a></li>
<li><a href="https://paperswithcode.com/" target="_blank" rel="noopener noreferrer">Papers with Code</a></li>
<li><a href="https://research.google/blog/" target="_blank" rel="noopener noreferrer">Google Research Blog</a></li>
<li><a href="https://www.microsoft.com/en-us/research/blog/" target="_blank" rel="noopener noreferrer">Microsoft Research Blog</a></li>
<li><a href="https://www.qualcomm.com/research/artificial-intelligence" target="_blank" rel="noopener noreferrer">Qualcomm AI Research</a></li>
<li><a href="https://ai.google.dev/edge" target="_blank" rel="noopener noreferrer">Google AI Edge</a></li>
<li><a href="https://huggingface.co/spaces/opencompass/open_vlm_leaderboard" target="_blank" rel="noopener noreferrer">Open VLM Leaderboard</a></li>
      </ul>
    </section>

    <footer>
      <div>요약은 Gemini API를 사용하며, 실제 원문은 "원문 보기" 링크를 확인하세요.</div>
      <div>중복 방지 기준: URL 기준 + 최근 180일 기록 유지</div>
    </footer>
  </div>
</body>
</html>
