<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>2026-02-20 AI 리서치 브리핑</title>
  <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;600;700&family=Noto+Sans+KR:wght@400;500;700&display=swap" rel="stylesheet">
  <style>
    :root {
      --bg: #f6f1ea;
      --bg-accent: #e3efe9;
      --card: #ffffff;
      --text: #1c232b;
      --muted: #5e6b78;
      --accent: #00808f;
      --accent-2: #ff7a59;
      --border: #e2dcd5;
      --shadow: 0 16px 40px rgba(24, 32, 45, 0.08);
    }

    * {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      font-family: 'Noto Sans KR', sans-serif;
      color: var(--text);
      background: radial-gradient(circle at top left, var(--bg-accent), var(--bg)) no-repeat;
      min-height: 100vh;
    }

    .page {
      max-width: 1180px;
      margin: 0 auto;
      padding: 48px 20px 64px;
      position: relative;
    }

    .hero {
      display: grid;
      gap: 16px;
      margin-bottom: 36px;
    }

    .hero h1 {
      font-family: 'Space Grotesk', sans-serif;
      font-size: clamp(2.2rem, 3vw, 3.4rem);
      margin: 0;
      letter-spacing: -0.02em;
    }

    .hero p {
      margin: 0;
      color: var(--muted);
      font-size: 1.05rem;
      max-width: 640px;
    }

    .hero-meta {
      display: flex;
      flex-wrap: wrap;
      gap: 12px;
      font-size: 0.95rem;
    }

    .hero-meta span {
      background: var(--card);
      border: 1px solid var(--border);
      padding: 6px 12px;
      border-radius: 999px;
    }

    .nav-back {
      text-decoration: none;
      color: var(--accent);
      font-weight: 600;
      display: inline-flex;
      align-items: center;
      gap: 8px;
    }

    .section-block {
      margin-bottom: 42px;
    }

    .section-header {
      margin-bottom: 18px;
    }

    .section-header h2 {
      font-family: 'Space Grotesk', sans-serif;
      margin: 0 0 6px;
      font-size: 1.6rem;
    }

    .section-header p {
      margin: 0;
      color: var(--muted);
    }

    .card-grid {
      display: grid;
      gap: 18px;
      grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
    }

    .card-grid.scrollable {
      max-height: 520px;
      overflow-y: auto;
      padding-right: 6px;
    }

    .card-grid.scrollable::-webkit-scrollbar {
      width: 8px;
    }

    .card-grid.scrollable::-webkit-scrollbar-track {
      background: rgba(0, 0, 0, 0.04);
      border-radius: 999px;
    }

    .card-grid.scrollable::-webkit-scrollbar-thumb {
      background: rgba(0, 128, 143, 0.35);
      border-radius: 999px;
    }

    .card {
      background: var(--card);
      border-radius: 18px;
      padding: 20px 20px 18px;
      box-shadow: var(--shadow);
      border: 1px solid var(--border);
      display: flex;
      flex-direction: column;
      gap: 12px;
    }

    .card h3 {
      font-size: 1.1rem;
      margin: 0;
      line-height: 1.4;
    }

    .card-meta {
      display: flex;
      flex-wrap: wrap;
      gap: 8px;
      font-size: 0.8rem;
      color: var(--muted);
    }

    .pill {
      background: rgba(0, 128, 143, 0.12);
      color: var(--accent);
      padding: 4px 10px;
      border-radius: 999px;
      font-weight: 600;
    }

    .card-summary {
      margin: 0;
      color: var(--text);
      line-height: 1.6;
    }

    .card-translation {
      margin: 0;
      background: rgba(255, 122, 89, 0.12);
      color: #7a2c17;
      padding: 10px 12px;
      border-radius: 12px;
      font-size: 0.92rem;
    }

    .card-translation span {
      font-weight: 700;
      margin-right: 6px;
    }

    .card-link {
      margin-top: auto;
      text-decoration: none;
      color: var(--accent);
      font-weight: 600;
      display: inline-flex;
      align-items: center;
      gap: 6px;
    }

    .sources {
      background: var(--card);
      border: 1px solid var(--border);
      border-radius: 18px;
      padding: 22px;
      box-shadow: var(--shadow);
    }

    .sources h2 {
      font-family: 'Space Grotesk', sans-serif;
      margin: 0 0 12px;
    }

    .sources ul {
      list-style: none;
      padding: 0;
      margin: 0;
      display: grid;
      gap: 8px;
      grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
    }

    .sources a {
      text-decoration: none;
      color: var(--accent);
      font-weight: 600;
    }

    footer {
      margin-top: 42px;
      color: var(--muted);
      font-size: 0.9rem;
      display: flex;
      flex-direction: column;
      gap: 6px;
      text-align: center;
    }

    @media (max-width: 720px) {
      .page {
        padding: 36px 16px 56px;
      }

      .hero-meta {
        flex-direction: column;
        align-items: flex-start;
      }
    }
  </style>
</head>
<body>
  <div class="page">
    <header class="hero">
      <a class="nav-back" href="../index.html">← 전체 목록</a>
      <h1>2026-02-20 AI 리서치 브리핑</h1>
      <p>최신 VLM, sLLM, on-device AI 논문과 연구 블로그를 한눈에 정리합니다. 중복 기사 방지를 위해 URL 기준으로 추적합니다.</p>
      <div class="hero-meta">
        <span>총 11건 요약</span>
        <span>자동 생성</span>
      </div>
    </header>

    
    <section class="section-block">
      <div class="section-header">
        <h2>VLM 업데이트</h2>
        <p>멀티모달 비전-언어 모델의 최신 논문과 리더보드 변화</p>
      </div>
      <div class="card-grid scrollable">
        
    <article class="card" data-item-url="http://arxiv.org/abs/2602.16702v1" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models (LLMs), achieving similar scaling in VLMs remains challenging. A key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominated and allowing early visual grounding errors to accumulate. Moreover, vanilla guidance for visual grounding during inference is often coarse and noisy, making it difficult to steer reasoning over long texts. To address these challenges, we propose \emph{Saliency-Aware Principle} (SAP) selection. SAP operates on high-level reasoning principles rather than token-level trajectories, which enable stable control over discrete generation under noisy feedback while allowing later reasoning steps to re-consult visual evidence when renewed grounding is required. In addition, SAP supports multi-route inference, enabling parallel exploration of diverse reasoning behaviors. SAP is model-agnostic and data-free, requiring no additional training. Empirical results show that SAP achieves competitive performance, especially in reducing object hallucination, under comparable token-generation budgets while yielding more stable reasoning and lower response latency than CoT-style long sequential reasoning.</p>
      
      <a class="card-link" href="http://arxiv.org/abs/2602.16702v1" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="http://arxiv.org/abs/2411.11706v4" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>MC-LLaVA: Multi-Concept Personalized Vision-Language Model</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">Current vision-language models (VLMs) show exceptional abilities across diverse tasks, such as visual question answering. To enhance user experience, recent studies have investigated VLM personalization to understand user-provided concepts. However, they mainly focus on single concepts, neglecting the existence and interplay of multiple concepts, which limits real-world applicability. This paper proposes MC-LLaVA, a multi-concept personalization paradigm. Specifically, MC-LLaVA employs a multi-concept instruction tuning strategy, effectively integrating multiple concepts in a single training step. To reduce the training costs, we propose a personalized textual prompt that uses visual token information to initialize concept tokens. Additionally, we introduce a personalized visual prompt during inference, aggregating location maps for enhanced recognition and grounding capabilities. To further push the performance upper bound, we incorporate an optional auxiliary loss, better enhancing the proposed personalized prompts. To decorate the VLM personalization research, we contribute a high-quality dataset. We carefully collect images with multiple characters and objects from movies and manually create question-answer samples for multi-concept scenarios, featuring superior diversity. Comprehensive experiments demonstrate that MC-LLaVA achieves impressive multi-concept personalized responses, paving the way for VLMs to become better user assistants. The code and dataset will be released at \href{https://github.com/arctanxarc/MC-LLaVA}{https://github.com/arctanxarc/MC-LLaVA}.</p>
      
      <a class="card-link" href="http://arxiv.org/abs/2411.11706v4" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  
      </div>
    </section>
  


    <section class="section-block">
      <div class="section-header">
        <h2>On-Device AI</h2>
        <p>디바이스 내 추론 및 엣지 최적화 동향</p>
      </div>
      <div class="card-grid scrollable">
        
    <article class="card" data-item-url="http://arxiv.org/abs/2602.16705v1" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.</p>
      
      <a class="card-link" href="http://arxiv.org/abs/2602.16705v1" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  
      </div>
    </section>
  

    <section class="section-block">
      <div class="section-header">
        <h2>AI 뉴스 &amp; 리서치</h2>
        <p>기업/연구기관의 주요 발표와 블로그 업데이트</p>
      </div>
      <div class="card-grid scrollable">
        
    <article class="card" data-item-url="https://huggingface.co/papers/2602.12675" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>SLA2: Sparse-Linear Attention with Learnable Routing and QAT</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">SLA2: Sparse-Linear Attention with Learnable Routing and QAT에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://huggingface.co/papers/2602.12675" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://huggingface.co/papers/2602.14979" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>RynnBrain: Open Embodied Foundation Models</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">RynnBrain: Open Embodied Foundation Models에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://huggingface.co/papers/2602.14979" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://huggingface.co/papers/2602.16705" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://huggingface.co/papers/2602.16705" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://huggingface.co/papers/2602.16317" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>CADEvolve: Creating Realistic CAD via Program Evolution</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">CADEvolve: Creating Realistic CAD via Program Evolution에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://huggingface.co/papers/2602.16317" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://huggingface.co/papers/2602.14080" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality에 관한 최근 업데이트입니다. 자세한 내용은 원문 링크에서 확인할 수 있습니다.</p>
      
      <a class="card-link" href="https://huggingface.co/papers/2602.14080" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="http://arxiv.org/abs/2602.16711v1" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>TeCoNeRV: Leveraging Temporal Coherence for Compressible Neural Representations for Videos</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">Implicit Neural Representations (INRs) have recently demonstrated impressive performance for video compression. However, since a separate INR must be overfit for each video, scaling to high-resolution videos while maintaining encoding efficiency remains a significant challenge. Hypernetwork-based approaches predict INR weights (hyponetworks) for unseen videos at high speeds, but with low quality, large compressed size, and prohibitive memory needs at higher resolutions. We address these fundamental limitations through three key contributions: (1) an approach that decomposes the weight prediction task spatially and temporally, by breaking short video segments into patch tubelets, to reduce the pretraining memory overhead by 20$\times$; (2) a residual-based storage scheme that captures only differences between consecutive segment representations, significantly reducing bitstream size; and (3) a temporal coherence regularization framework that encourages changes in the weight space to be correlated with video content. Our proposed method, TeCoNeRV, achieves substantial improvements of 2.47dB and 5.35dB PSNR over the baseline at 480p and 720p on UVG, with 36% lower bitrates and 1.5-3$\times$ faster encoding speeds. With our low memory usage, we are the first hypernetwork approach to demonstrate results at 480p, 720p and 1080p on UVG, HEVC and MCL-JCV. Our project page is available at https://namithap10.github.io/teconerv/ .</p>
      
      <a class="card-link" href="http://arxiv.org/abs/2602.16711v1" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="http://arxiv.org/abs/2602.16689v1" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>Are Object-Centric Representations Better At Compositional Generalization?</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">Compositional generalization, the ability to reason about novel combinations of familiar concepts, is fundamental to human cognition and a critical challenge for machine learning. Object-centric (OC) representations, which encode a scene as a set of objects, are often argued to support such generalization, but systematic evidence in visually rich settings is limited. We introduce a Visual Question Answering benchmark across three controlled visual worlds (CLEVRTex, Super-CLEVR, and MOVi-C) to measure how well vision encoders, with and without object-centric biases, generalize to unseen combinations of object properties. To ensure a fair and comprehensive comparison, we carefully account for training data diversity, sample size, representation size, downstream model capacity, and compute. We use DINOv2 and SigLIP2, two widely used vision encoders, as the foundation models and their OC counterparts. Our key findings reveal that (1) OC approaches are superior in harder compositional generalization settings; (2) original dense representations surpass OC only on easier settings and typically require substantially more downstream compute; and (3) OC models are more sample efficient, achieving stronger generalization with fewer images, whereas dense encoders catch up or surpass them only with sufficient data and diversity. Overall, object-centric representations offer stronger compositional generalization when any one of dataset size, training data diversity, or downstream compute is constrained.</p>
      
      <a class="card-link" href="http://arxiv.org/abs/2602.16689v1" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://www.microsoft.com/en-us/research/blog/media-authenticity-methods-in-practice-capabilities-limitations-and-directions" data-source="Microsoft Research Blog" data-item-type="news">
      <header>
        <h3>Media Authenticity Methods in Practice: Capabilities, Limitations, and Directions</h3>
        <div class="card-meta">
          <span class="pill">News</span>
          <span class="source">Microsoft Research Blog</span>
        </div>
      </header>
      <p class="card-summary">As synthetic media grows, verifying what’s real, and the origin of content, matters more than ever. Our latest report explores media integrity and authentication methods, their limits, and practical paths toward trustworthy provenance across images, audio, and video.</p>
      
      <a class="card-link" href="https://www.microsoft.com/en-us/research/blog/media-authenticity-methods-in-practice-capabilities-limitations-and-directions" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  
      </div>
    </section>
  

    <section class="sources">
      <h2>참고한 소스</h2>
      <ul>
        <li><a href="https://huggingface.co/papers" target="_blank" rel="noopener noreferrer">Hugging Face Papers</a></li>
<li><a href="https://export.arxiv.org/api/query?search_query=cat:cs.CV&amp;sortBy=lastUpdatedDate&amp;sortOrder=descending&amp;max_results=10" target="_blank" rel="noopener noreferrer">arXiv cs.CV (recent)</a></li>
<li><a href="https://paperswithcode.com/" target="_blank" rel="noopener noreferrer">Papers with Code</a></li>
<li><a href="https://research.google/blog/" target="_blank" rel="noopener noreferrer">Google Research Blog</a></li>
<li><a href="https://www.microsoft.com/en-us/research/blog/" target="_blank" rel="noopener noreferrer">Microsoft Research Blog</a></li>
<li><a href="https://www.qualcomm.com/research/artificial-intelligence" target="_blank" rel="noopener noreferrer">Qualcomm AI Research</a></li>
<li><a href="https://ai.google.dev/edge" target="_blank" rel="noopener noreferrer">Google AI Edge</a></li>
<li><a href="https://huggingface.co/spaces/opencompass/open_vlm_leaderboard" target="_blank" rel="noopener noreferrer">Open VLM Leaderboard</a></li>
      </ul>
    </section>

    <footer>
      <div>요약은 Gemini API를 사용하며, 실제 원문은 "원문 보기" 링크를 확인하세요.</div>
      <div>중복 방지 기준: URL 기준 + 최근 180일 기록 유지</div>
    </footer>
  </div>
</body>
</html>
