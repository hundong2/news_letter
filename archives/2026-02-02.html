<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>2026-02-02 AI 리서치 브리핑</title>
  <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;600;700&family=Noto+Sans+KR:wght@400;500;700&display=swap" rel="stylesheet">
  <style>
    :root {
      --bg: #f6f1ea;
      --bg-accent: #e3efe9;
      --card: #ffffff;
      --text: #1c232b;
      --muted: #5e6b78;
      --accent: #00808f;
      --accent-2: #ff7a59;
      --border: #e2dcd5;
      --shadow: 0 16px 40px rgba(24, 32, 45, 0.08);
    }

    * {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      font-family: 'Noto Sans KR', sans-serif;
      color: var(--text);
      background: radial-gradient(circle at top left, var(--bg-accent), var(--bg)) no-repeat;
      min-height: 100vh;
    }

    .page {
      max-width: 1180px;
      margin: 0 auto;
      padding: 48px 20px 64px;
      position: relative;
    }

    .hero {
      display: grid;
      gap: 16px;
      margin-bottom: 36px;
    }

    .hero h1 {
      font-family: 'Space Grotesk', sans-serif;
      font-size: clamp(2.2rem, 3vw, 3.4rem);
      margin: 0;
      letter-spacing: -0.02em;
    }

    .hero p {
      margin: 0;
      color: var(--muted);
      font-size: 1.05rem;
      max-width: 640px;
    }

    .hero-meta {
      display: flex;
      flex-wrap: wrap;
      gap: 12px;
      font-size: 0.95rem;
    }

    .hero-meta span {
      background: var(--card);
      border: 1px solid var(--border);
      padding: 6px 12px;
      border-radius: 999px;
    }

    .nav-back {
      text-decoration: none;
      color: var(--accent);
      font-weight: 600;
      display: inline-flex;
      align-items: center;
      gap: 8px;
    }

    .section-block {
      margin-bottom: 42px;
    }

    .section-header {
      margin-bottom: 18px;
    }

    .section-header h2 {
      font-family: 'Space Grotesk', sans-serif;
      margin: 0 0 6px;
      font-size: 1.6rem;
    }

    .section-header p {
      margin: 0;
      color: var(--muted);
    }

    .card-grid {
      display: grid;
      gap: 18px;
      grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
    }

    .card {
      background: var(--card);
      border-radius: 18px;
      padding: 20px 20px 18px;
      box-shadow: var(--shadow);
      border: 1px solid var(--border);
      display: flex;
      flex-direction: column;
      gap: 12px;
    }

    .card h3 {
      font-size: 1.1rem;
      margin: 0;
      line-height: 1.4;
    }

    .card-meta {
      display: flex;
      flex-wrap: wrap;
      gap: 8px;
      font-size: 0.8rem;
      color: var(--muted);
    }

    .pill {
      background: rgba(0, 128, 143, 0.12);
      color: var(--accent);
      padding: 4px 10px;
      border-radius: 999px;
      font-weight: 600;
    }

    .card-summary {
      margin: 0;
      color: var(--text);
      line-height: 1.6;
    }

    .card-translation {
      margin: 0;
      background: rgba(255, 122, 89, 0.12);
      color: #7a2c17;
      padding: 10px 12px;
      border-radius: 12px;
      font-size: 0.92rem;
    }

    .card-translation span {
      font-weight: 700;
      margin-right: 6px;
    }

    .card-link {
      margin-top: auto;
      text-decoration: none;
      color: var(--accent);
      font-weight: 600;
      display: inline-flex;
      align-items: center;
      gap: 6px;
    }

    .sources {
      background: var(--card);
      border: 1px solid var(--border);
      border-radius: 18px;
      padding: 22px;
      box-shadow: var(--shadow);
    }

    .sources h2 {
      font-family: 'Space Grotesk', sans-serif;
      margin: 0 0 12px;
    }

    .sources ul {
      list-style: none;
      padding: 0;
      margin: 0;
      display: grid;
      gap: 8px;
      grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
    }

    .sources a {
      text-decoration: none;
      color: var(--accent);
      font-weight: 600;
    }

    footer {
      margin-top: 42px;
      color: var(--muted);
      font-size: 0.9rem;
      display: flex;
      flex-direction: column;
      gap: 6px;
      text-align: center;
    }

    @media (max-width: 720px) {
      .page {
        padding: 36px 16px 56px;
      }

      .hero-meta {
        flex-direction: column;
        align-items: flex-start;
      }
    }
  </style>
</head>
<body>
  <div class="page">
    <header class="hero">
      <a class="nav-back" href="../index.html">← 전체 목록</a>
      <h1>2026-02-02 AI 리서치 브리핑</h1>
      <p>최신 VLM, sLLM, on-device AI 논문과 연구 블로그를 한눈에 정리합니다. 중복 기사 방지를 위해 URL 기준으로 추적합니다.</p>
      <div class="hero-meta">
        <span>총 18건 요약</span>
        <span>자동 생성</span>
      </div>
    </header>

    



    <section class="section-block">
      <div class="section-header">
        <h2>AI 뉴스 &amp; 리서치</h2>
        <p>기업/연구기관의 주요 발표와 블로그 업데이트</p>
      </div>
      <div class="card-grid">
        
    <article class="card" data-item-url="https://huggingface.co/papers/2601.21558" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">Join the discussion on this paper page</p>
      <p class="card-translation"><span>English</span> Summary unavailable.</p>
      <a class="card-link" href="https://huggingface.co/papers/2601.21558" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://huggingface.co/papers/2601.23143" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>THINKSAFE: Self-Generated Safety Alignment for Reasoning Models</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">Join the discussion on this paper page</p>
      <p class="card-translation"><span>English</span> Summary unavailable.</p>
      <a class="card-link" href="https://huggingface.co/papers/2601.23143" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://huggingface.co/papers/2601.22628" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>TTCS: Test-Time Curriculum Synthesis for Self-Evolving</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">Join the discussion on this paper page</p>
      <p class="card-translation"><span>English</span> Summary unavailable.</p>
      <a class="card-link" href="https://huggingface.co/papers/2601.22628" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://huggingface.co/papers/2601.23265" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>PaperBanana: Automating Academic Illustration for AI Scientists</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">Join the discussion on this paper page</p>
      <p class="card-translation"><span>English</span> Summary unavailable.</p>
      <a class="card-link" href="https://huggingface.co/papers/2601.23265" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://huggingface.co/papers/2601.23184" data-source="Hugging Face Papers" data-item-type="paper">
      <header>
        <h3>ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">Hugging Face Papers</span>
        </div>
      </header>
      <p class="card-summary">Join the discussion on this paper page</p>
      <p class="card-translation"><span>English</span> Summary unavailable.</p>
      <a class="card-link" href="https://huggingface.co/papers/2601.23184" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="http://arxiv.org/abs/2601.23286v1" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>VideoGPA: Distilling Geometry Priors for 3D-Consistent Video Generation</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">While recent video diffusion models (VDMs) produce visually impressive results, they fundamentally struggle to maintain 3D structural consistency, often resulting in object deformation or spatial drift. We hypothesize that these failures arise because standard denoising objectives lack explicit incentives for geometric coherence. To address this, we introduce VideoGPA (Video Geometric Preference Alignment), a data-efficient self-supervised framework that leverages a geometry foundation model to automatically derive dense preference signals that guide VDMs via Direct Preference Optimization (DPO). This approach effectively steers the generative distribution toward inherent 3D consistency without requiring human annotations. VideoGPA significantly enhances temporal stability, physical plausibility, and motion coherence using minimal preference pairs, consistently outperforming state-of-the-art baselines in extensive experiments.</p>
      <p class="card-translation"><span>English</span> Summary unavailable.</p>
      <a class="card-link" href="http://arxiv.org/abs/2601.23286v1" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="http://arxiv.org/abs/2601.18532v2" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>From Cold Start to Active Learning: Embedding-Based Scan Selection for Medical Image Segmentation</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">Accurate segmentation annotations are critical for disease monitoring, yet manual labeling remains a major bottleneck due to the time and expertise required. Active learning (AL) alleviates this burden by prioritizing informative samples for annotation, typically through a diversity-based cold-start phase followed by uncertainty-driven selection. We propose a novel cold-start sampling strategy that combines foundation-model embeddings with clustering, including automatic selection of the number of clusters and proportional sampling across clusters, to construct a diverse and representative initial training. This is followed by an uncertainty-based AL framework that integrates spatial diversity to guide sample selection. The proposed method is intuitive and interpretable, enabling visualization of the feature-space distribution of candidate samples. We evaluate our approach on three datasets spanning X-ray and MRI modalities. On the CheXmask dataset, the cold-start strategy outperforms random selection, improving Dice from 0.918 to 0.929 and reducing the Hausdorff distance from 32.41 to 27.66 mm. In the AL setting, combined entropy and diversity selection improves Dice from 0.919 to 0.939 and reduces the Hausdorff distance from 30.10 to 19.16 mm. On the Montgomery dataset, cold-start gains are substantial, with Dice improving from 0.928 to 0.950 and Hausdorff distance decreasing from 14.22 to 9.38 mm. On the SynthStrip dataset, cold-start selection slightly affects Dice but reduces the Hausdorff distance from 9.43 to 8.69 mm, while active learning improves Dice from 0.816 to 0.826 and reduces the Hausdorff distance from 7.76 to 6.38 mm. Overall, the proposed framework consistently outperforms baseline methods in low-data regimes, improving segmentation accuracy.</p>
      <p class="card-translation"><span>English</span> Summary unavailable.</p>
      <a class="card-link" href="http://arxiv.org/abs/2601.18532v2" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="http://arxiv.org/abs/2601.23281v1" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>User Prompting Strategies and Prompt Enhancement Methods for Open-Set Object Detection in XR Environments</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">Open-set object detection (OSOD) localizes objects while identifying and rejecting unknown classes at inference. While recent OSOD models perform well on benchmarks, their behavior under realistic user prompting remains underexplored. In interactive XR settings, user-generated prompts are often ambiguous, underspecified, or overly detailed. To study prompt-conditioned robustness, we evaluate two OSOD models, GroundingDINO and YOLO-E, on real-world XR images and simulate diverse user prompting behaviors using vision-language models. We consider four prompt types: standard, underdetailed, overdetailed, and pragmatically ambiguous, and examine the impact of two enhancement strategies on these prompts. Results show that both models exhibit stable performance under underdetailed and standard prompts, while they suffer degradation under ambiguous prompts. Overdetailed prompts primarily affect GroundingDINO. Prompt enhancement substantially improves robustness under ambiguity, yielding gains exceeding 55% mIoU and 41% average confidence. Based on the findings, we propose several prompting strategies and prompt enhancement methods for OSOD models in XR environments.</p>
      <p class="card-translation"><span>English</span> Summary unavailable.</p>
      <a class="card-link" href="http://arxiv.org/abs/2601.23281v1" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="http://arxiv.org/abs/2601.23276v1" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>Denoising the Deep Sky: Physics-Based CCD Noise Formation for Astronomical Imaging</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">Astronomical imaging remains noise-limited under practical observing constraints, while standard calibration pipelines mainly remove structured artifacts and leave stochastic noise largely unresolved. Learning-based denoising is promising, yet progress is hindered by scarce paired training data and the need for physically interpretable and reproducible models in scientific workflows. We propose a physics-based noise synthesis framework tailored to CCD noise formation. The pipeline models photon shot noise, photo-response non-uniformity, dark-current noise, readout effects, and localized outliers arising from cosmic-ray hits and hot pixels. To obtain low-noise inputs for synthesis, we average multiple unregistered exposures to produce high-SNR bases. Realistic noisy counterparts synthesized from these bases using our noise model enable the construction of abundant paired datasets for supervised learning. We further introduce a real-world dataset across multi-bands acquired with two twin ground-based telescopes, providing paired raw frames and instrument-pipeline calibrated frames, together with calibration data and stacked high-SNR bases for real-world evaluation.</p>
      <p class="card-translation"><span>English</span> Summary unavailable.</p>
      <a class="card-link" href="http://arxiv.org/abs/2601.23276v1" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="http://arxiv.org/abs/2601.23265v1" data-source="arXiv cs.CV (recent)" data-item-type="paper">
      <header>
        <h3>PaperBanana: Automating Academic Illustration for AI Scientists</h3>
        <div class="card-meta">
          <span class="pill">Paper</span>
          <span class="source">arXiv cs.CV (recent)</span>
        </div>
      </header>
      <p class="card-summary">Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations.</p>
      <p class="card-translation"><span>English</span> Summary unavailable.</p>
      <a class="card-link" href="http://arxiv.org/abs/2601.23265v1" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://research.google/blog/towards-a-science-of-scaling-agent-systems-when-and-why-agent-systems-work" data-source="Google Research Blog" data-item-type="news">
      <header>
        <h3>January 28, 2026 Towards a science of scaling agent systems: When and why agent systems work Generative AI &amp;#183; Machine Intelligence</h3>
        <div class="card-meta">
          <span class="pill">News</span>
          <span class="source">Google Research Blog</span>
        </div>
      </header>
      <p class="card-summary">January 28, 2026 Towards a science of scaling agent systems: When and why agent systems work Generative AI &amp;#183; Machine Intelligence 관련 업데이트입니다.</p>
      
      <a class="card-link" href="https://research.google/blog/towards-a-science-of-scaling-agent-systems-when-and-why-agent-systems-work" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://research.google/blog/label/algorithms-theory" data-source="Google Research Blog" data-item-type="news">
      <header>
        <h3>Algorithms &amp; Theory</h3>
        <div class="card-meta">
          <span class="pill">News</span>
          <span class="source">Google Research Blog</span>
        </div>
      </header>
      <p class="card-summary">Algorithms &amp; Theory 관련 업데이트입니다.</p>
      
      <a class="card-link" href="https://research.google/blog/label/algorithms-theory" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://research.google/blog/label/climate-sustainability" data-source="Google Research Blog" data-item-type="news">
      <header>
        <h3>Climate &amp; Sustainability</h3>
        <div class="card-meta">
          <span class="pill">News</span>
          <span class="source">Google Research Blog</span>
        </div>
      </header>
      <p class="card-summary">Climate &amp; Sustainability 관련 업데이트입니다.</p>
      
      <a class="card-link" href="https://research.google/blog/label/climate-sustainability" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://research.google/blog/label/conferences-events" data-source="Google Research Blog" data-item-type="news">
      <header>
        <h3>Conferences &amp; Events</h3>
        <div class="card-meta">
          <span class="pill">News</span>
          <span class="source">Google Research Blog</span>
        </div>
      </header>
      <p class="card-summary">Conferences &amp; Events 관련 업데이트입니다.</p>
      
      <a class="card-link" href="https://research.google/blog/label/conferences-events" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://research.google/blog/label/data-management" data-source="Google Research Blog" data-item-type="news">
      <header>
        <h3>Data Management</h3>
        <div class="card-meta">
          <span class="pill">News</span>
          <span class="source">Google Research Blog</span>
        </div>
      </header>
      <p class="card-summary">Data Management 관련 업데이트입니다.</p>
      
      <a class="card-link" href="https://research.google/blog/label/data-management" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://www.microsoft.com/en-us/research/blog" data-source="Microsoft Research Blog" data-item-type="news">
      <header>
        <h3>Microsoft Research blog</h3>
        <div class="card-meta">
          <span class="pill">News</span>
          <span class="source">Microsoft Research Blog</span>
        </div>
      </header>
      <p class="card-summary">Microsoft Research blog 관련 업데이트입니다.</p>
      
      <a class="card-link" href="https://www.microsoft.com/en-us/research/blog" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://www.microsoft.com/en-us/research/blog/unirg-scaling-medical-imaging-report-generation-with-multimodal-reinforcement-learning" data-source="Microsoft Research Blog" data-item-type="news">
      <header>
        <h3>UniRG: Scaling medical imaging report generation with multimodal reinforcement learning</h3>
        <div class="card-meta">
          <span class="pill">News</span>
          <span class="source">Microsoft Research Blog</span>
        </div>
      </header>
      <p class="card-summary">UniRG: Scaling medical imaging report generation with multimodal reinforcement learning 관련 업데이트입니다.</p>
      
      <a class="card-link" href="https://www.microsoft.com/en-us/research/blog/unirg-scaling-medical-imaging-report-generation-with-multimodal-reinforcement-learning" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  

    <article class="card" data-item-url="https://www.microsoft.com/en-us/research/blog/multimodal-reinforcement-learning-with-agentic-verifier-for-ai-agents" data-source="Microsoft Research Blog" data-item-type="news">
      <header>
        <h3>Multimodal reinforcement learning with agentic verifier for AI agents</h3>
        <div class="card-meta">
          <span class="pill">News</span>
          <span class="source">Microsoft Research Blog</span>
        </div>
      </header>
      <p class="card-summary">Multimodal reinforcement learning with agentic verifier for AI agents 관련 업데이트입니다.</p>
      
      <a class="card-link" href="https://www.microsoft.com/en-us/research/blog/multimodal-reinforcement-learning-with-agentic-verifier-for-ai-agents" target="_blank" rel="noopener noreferrer">원문 보기</a>
    </article>
  
      </div>
    </section>
  

    <section class="sources">
      <h2>참고한 소스</h2>
      <ul>
        <li><a href="https://huggingface.co/papers" target="_blank" rel="noopener noreferrer">Hugging Face Papers</a></li>
<li><a href="https://export.arxiv.org/api/query?search_query=cat:cs.CV&amp;sortBy=lastUpdatedDate&amp;sortOrder=descending&amp;max_results=10" target="_blank" rel="noopener noreferrer">arXiv cs.CV (recent)</a></li>
<li><a href="https://paperswithcode.com/" target="_blank" rel="noopener noreferrer">Papers with Code</a></li>
<li><a href="https://research.google/blog/" target="_blank" rel="noopener noreferrer">Google Research Blog</a></li>
<li><a href="https://www.microsoft.com/en-us/research/blog/" target="_blank" rel="noopener noreferrer">Microsoft Research Blog</a></li>
<li><a href="https://www.qualcomm.com/research/artificial-intelligence" target="_blank" rel="noopener noreferrer">Qualcomm AI Research</a></li>
<li><a href="https://ai.google.dev/edge" target="_blank" rel="noopener noreferrer">Google AI Edge</a></li>
<li><a href="https://huggingface.co/spaces/opencompass/open_vlm_leaderboard" target="_blank" rel="noopener noreferrer">Open VLM Leaderboard</a></li>
      </ul>
    </section>

    <footer>
      <div>요약은 Gemini API를 사용하며, 실제 원문은 "원문 보기" 링크를 확인하세요.</div>
      <div>중복 방지 기준: URL 기준 + 최근 180일 기록 유지</div>
    </footer>
  </div>
</body>
</html>
